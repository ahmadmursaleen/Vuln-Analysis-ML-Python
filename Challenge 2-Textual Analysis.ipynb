{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('yelp_data_train.dat', header=0, \\\n",
    "                    delimiter=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['date;reviewID;reviewerID;reviewContent;rating;usefulCount;coolCount;funnyCount;fake;hotelID'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = np.array(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=train_array[0][0].split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = np.array(var)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9/16/2010', 'Ol', 'nf3q2h-kSQoZK2jBY92FOg',\n",
       "       '\"If you are considering staying here, watch this first: http://www.youtube.com/w…\"',\n",
       "       '1', '8', '2', '6', 'N', 'tQfLGoolUMu2J0igcWcoZg'], dtype='<U82')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = []\n",
    "for i in range(2908):\n",
    "    var=train_array[i][0].split(';')\n",
    "    var=np.array(var)\n",
    "    #rev.append(var[3])\n",
    "    rev.append(var[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = np.array(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range(2908):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( rev[i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908, 5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev2 = []\n",
    "for i in range(2908):\n",
    "    var=train_array[i][0].split(';')\n",
    "    var=np.array(var)\n",
    "    #rev.append(var[3])\n",
    "    rev2.append(var[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev2 = np.array(rev2)\n",
    "rev2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "y=0\n",
    "zeros=0\n",
    "ones=0\n",
    "for i in range(2908):\n",
    "    if (rev2[i] == 'N'):\n",
    "        n+=1\n",
    "    elif (rev2[i] == 'Y'):\n",
    "        y+=1\n",
    "    elif (rev2[i] == '0'):\n",
    "        zeros+=1\n",
    "    elif (rev2[i] == '1'):\n",
    "        ones+=1\n",
    "    #else:\n",
    "        #print(rev2[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print (\"Training the random forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features[:2000][:], rev2[:2000] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "YPred = forest.predict(train_data_features[2000:][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      " (5) Hotel lobby packed with a kid's convention\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "9\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "12\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "11\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "Err=0\n",
    "for i in range(908):\n",
    "    if YPred[i] != rev2[i+2000]:\n",
    "        print(rev2[i+2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv('yelp_data_train.dat', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1=str(df[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1=str1.split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9/16/2010',\n",
       " 'Ol',\n",
       " 'nf3q2h-kSQoZK2jBY92FOg',\n",
       " '\"If you are considering staying here, watch this first: http://www.youtube.com/w…\"',\n",
       " '1',\n",
       " '8',\n",
       " '2',\n",
       " '6',\n",
       " 'N',\n",
       " 'tQfLGoolUMu2J0igcWcoZg']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1[-2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"If you are considering staying here, watch this first: http://www.youtube.com/w…\"']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_test = []\n",
    "for i in range(2908):\n",
    "    var=df[i][0].split(';')\n",
    "    #var=np.array(var)\n",
    "    #rev.append(var[3])\n",
    "    rev_test.append(var[-2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_test = np.array(rev_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_test = rev_test.reshape(2908,)\n",
    "rev_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "y=0\n",
    "zeros=0\n",
    "ones=0\n",
    "for i in range(2908):\n",
    "    if (rev_test[i] == 'N'):\n",
    "        n+=1\n",
    "    elif (rev_test[i] == 'Y'):\n",
    "        y+=1\n",
    "    elif (rev_test[i] == '0'):\n",
    "        zeros+=1\n",
    "    elif (rev_test[i] == '1'):\n",
    "        ones+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2516"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_train = []\n",
    "for i in range(2908):\n",
    "    var=df[i][0].split(';')\n",
    "    #var=np.array(var)\n",
    "    #rev.append(var[3])\n",
    "    rev_train.append(var[3:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_id_train = []\n",
    "for i in range(2908):\n",
    "    var=df[i][0].split(';')\n",
    "    #var=np.array(var)\n",
    "    #rev.append(var[3])\n",
    "    rev_id_train.append(var[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_date_train = []\n",
    "for i in range(2908):\n",
    "    var=df[i][0].split(';')\n",
    "    #var=np.array(var)\n",
    "    #rev.append(var[3])\n",
    "    rev_date_train.append(var[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_date_train = np.array(rev_date_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_train = np.array(rev_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'N', 'N', ..., 'Y', 'Y', 'Y'], dtype='<U1')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range(2908):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( str(rev_train[i]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "#print (vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigram Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1) \n",
    "train_data_features1 = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features1 = train_data_features1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_bi = vectorizer.get_feature_names()\n",
    "#print (vocab_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908, 5000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908, 5000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2908x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 172979 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=False)\n",
    "train_data_features1 = tfidf_transformer.fit_transform(train_data_features1)\n",
    "train_data_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8542024013722127\n",
      "0.8625429553264605\n",
      "0.8657487091222031\n",
      "0.8450946643717728\n",
      "0.8588640275387264\n",
      "avg 0.857290551546275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "avg_score=0\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "    forest = forest.fit( X_train, y_train )\n",
    "    #YPred = forest.predict(X_test)\n",
    "    score = forest.score(X_test,y_test)\n",
    "    avg_score += score \n",
    "    #y.append(YPred)\n",
    "    print(score)\n",
    "    #y_test1=[]\n",
    "    #for ii in range(y_test.size):\n",
    "    #    if (y_test[ii] == 'N'):\n",
    "    #        y_test1.append(0)\n",
    "    #    else:\n",
    "    #        y_test1.append(1)\n",
    "    #YPred1=[]\n",
    "    #for ii in range(YPred.size):\n",
    "    #    if (YPred[ii] == 'N'):\n",
    "    #        YPred1.append(0)\n",
    "    #    else:\n",
    "    #        YPred1.append(1)    \n",
    "    #fpr, tpr, thresholds = metrics.roc_curve(y_test1, YPred1, pos_label=2)\n",
    "    #print(\"AUC\",metrics.auc(fpr, tpr))\n",
    "    #print(YPred.shape)\n",
    "    #Err=0\n",
    "    #for i in range(YPred.size):\n",
    "     #   if YPred[i] != y_test[i]:\n",
    "          #  Err+=1\n",
    "            #print(YPred[i], y_test[i])\n",
    "    #print (Err)\n",
    "print(\"avg\",avg_score/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8010291595197255\n",
      "0.7972508591065293\n",
      "0.8175559380378657\n",
      "0.7160068846815835\n",
      "0.7986230636833046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train, y_train)\n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(mnb.score(X_test,y_test))\n",
    "    #print(YPred.shape)\n",
    "    #Err=0\n",
    "    #for i in range(YPred.size):\n",
    "     #   if YPred[i] != y_test[i]:\n",
    "          #  Err+=1\n",
    "            #print(YPred[i], y_test[i])\n",
    "    #print (Err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7838765008576329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788659793814433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8416523235800344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8037865748709122\n",
      "0.7900172117039587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    PAC = PassiveAggressiveClassifier(n_iter=5)\n",
    "    PAC.fit(X_train, y_train)\n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(PAC.score(X_test,y_test))\n",
    "    #print(YPred.shape)\n",
    "    #Err=0\n",
    "    #for i in range(YPred.size):\n",
    "     #   if YPred[i] != y_test[i]:\n",
    "          #  Err+=1\n",
    "            #print(YPred[i], y_test[i])\n",
    "    #print (Err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7735849056603774\n",
      "0.7835051546391752\n",
      "0.8141135972461274\n",
      "0.7624784853700516\n",
      "0.774526678141136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(gnb.score(X_test,y_test))\n",
    "    #print(YPred.shape)\n",
    "    #Err=0\n",
    "    #for i in range(YPred.size):\n",
    "     #   if YPred[i] != y_test[i]:\n",
    "          #  Err+=1\n",
    "            #print(YPred[i], y_test[i])\n",
    "    #print (Err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8164665523156089\n",
      "0.7474226804123711\n",
      "0.8313253012048193\n",
      "0.7504302925989673\n",
      "0.8227194492254734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X_train, y_train)\n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(bnb.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8644939965694682\n",
      "0.8642611683848798\n",
      "0.8657487091222031\n",
      "0.8657487091222031\n",
      "0.8657487091222031\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(svm.SVC(kernel='rbf').fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7547169811320755\n",
      "0.7852233676975945\n",
      "0.8261617900172117\n",
      "0.7211703958691911\n",
      "0.7831325301204819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8644939965694682\n",
      "0.8625429553264605\n",
      "0.8657487091222031\n",
      "0.8640275387263339\n",
      "0.8640275387263339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "neigh = KNeighborsClassifier(n_neighbors=25)\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(neigh.fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8010291595197255\n",
      "0.8109965635738832\n",
      "0.8261617900172117\n",
      "0.7676419965576592\n",
      "0.8141135972461274\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(logreg.fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7324185248713551\n",
      "0.7525773195876289\n",
      "0.802065404475043\n",
      "0.693631669535284\n",
      "0.7641996557659209\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(svm.SVC(kernel='linear').fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8644939965694682\n",
      "0.8642611683848798\n",
      "0.8657487091222031\n",
      "0.8657487091222031\n",
      "0.8657487091222031\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(svm.SVC(kernel='sigmoid').fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7341337907375644\n",
      "0.7457044673539519\n",
      "0.802065404475043\n",
      "0.7056798623063684\n",
      "0.7624784853700516\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(svm.LinearSVC().fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing Bigram - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8644939965694682\n",
      "0.8642611683848798\n",
      "0.8657487091222031\n",
      "0.8657487091222031\n",
      "0.8657487091222031\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features1[train_index], train_data_features1[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(svm.SVC(kernel='sigmoid').fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing Bigram - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print (\"Parsing sentences from training set\")\n",
    "for i in range(2908):\n",
    "    sentences += review_to_sentences(rev[i], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29216,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-03 12:15:10,813 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2018-04-03 12:15:10,819 : INFO : collecting all words and their counts\n",
      "2018-04-03 12:15:10,819 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-04-03 12:15:10,858 : INFO : PROGRESS: at sentence #10000, processed 148705 words, keeping 8015 word types\n",
      "2018-04-03 12:15:10,902 : INFO : PROGRESS: at sentence #20000, processed 300514 words, keeping 11567 word types\n",
      "2018-04-03 12:15:10,946 : INFO : collected 13761 word types from a corpus of 443470 raw words and 29216 sentences\n",
      "2018-04-03 12:15:10,947 : INFO : Loading a fresh vocabulary\n",
      "2018-04-03 12:15:10,956 : INFO : min_count=40 retains 1077 unique words (7% of original 13761, drops 12684)\n",
      "2018-04-03 12:15:10,957 : INFO : min_count=40 leaves 386405 word corpus (87% of original 443470, drops 57065)\n",
      "2018-04-03 12:15:10,964 : INFO : deleting the raw counts dictionary of 13761 items\n",
      "2018-04-03 12:15:10,965 : INFO : sample=0.001 downsamples 67 most-common words\n",
      "2018-04-03 12:15:10,966 : INFO : downsampling leaves estimated 253508 word corpus (65.6% of prior 386405)\n",
      "2018-04-03 12:15:10,971 : INFO : estimated required memory for 1077 words and 300 dimensions: 3123300 bytes\n",
      "2018-04-03 12:15:10,972 : INFO : resetting layer weights\n",
      "2018-04-03 12:15:10,994 : INFO : training model with 4 workers on 1077 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-03 12:15:11,216 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-03 12:15:11,217 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-03 12:15:11,219 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-03 12:15:11,222 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-03 12:15:11,223 : INFO : EPOCH - 1 : training on 443470 raw words (253440 effective words) took 0.2s, 1150481 effective words/s\n",
      "2018-04-03 12:15:11,443 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-03 12:15:11,445 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-03 12:15:11,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-03 12:15:11,451 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-03 12:15:11,452 : INFO : EPOCH - 2 : training on 443470 raw words (253286 effective words) took 0.2s, 1162531 effective words/s\n",
      "2018-04-03 12:15:11,671 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-03 12:15:11,674 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-03 12:15:11,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-03 12:15:11,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-03 12:15:11,678 : INFO : EPOCH - 3 : training on 443470 raw words (253717 effective words) took 0.2s, 1183809 effective words/s\n",
      "2018-04-03 12:15:11,890 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-03 12:15:11,891 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-03 12:15:11,892 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-03 12:15:11,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-03 12:15:11,896 : INFO : EPOCH - 4 : training on 443470 raw words (253326 effective words) took 0.2s, 1220789 effective words/s\n",
      "2018-04-03 12:15:12,133 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-03 12:15:12,137 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-03 12:15:12,140 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-03 12:15:12,142 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-03 12:15:12,142 : INFO : EPOCH - 5 : training on 443470 raw words (253647 effective words) took 0.2s, 1099461 effective words/s\n",
      "2018-04-03 12:15:12,143 : INFO : training on a 2217350 raw words (1267416 effective words) took 1.1s, 1103606 effective words/s\n",
      "2018-04-03 12:15:12,144 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-04-03 12:15:12,156 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-04-03 12:15:12,157 : INFO : not storing attribute vectors_norm\n",
      "2018-04-03 12:15:12,158 : INFO : not storing attribute cum_table\n",
      "2018-04-03 12:15:12,199 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x111f2ea90>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elevator', 0.9163681864738464),\n",
       " ('down', 0.9068641066551208),\n",
       " ('into', 0.788445770740509),\n",
       " ('ice', 0.7834097146987915),\n",
       " ('set', 0.7741348147392273),\n",
       " ('cold', 0.7718610763549805),\n",
       " ('doors', 0.7655811309814453),\n",
       " ('hall', 0.7634575366973877),\n",
       " ('open', 0.7625901699066162),\n",
       " ('off', 0.7540481090545654)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.most_similar(\"door\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-03 12:15:17,295 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2018-04-03 12:15:17,316 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2018-04-03 12:15:17,317 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-04-03 12:15:17,318 : INFO : loading vocabulary recursively from 300features_40minwords_10context.vocabulary.* with mmap=None\n",
      "2018-04-03 12:15:17,318 : INFO : loading trainables recursively from 300features_40minwords_10context.trainables.* with mmap=None\n",
      "2018-04-03 12:15:17,319 : INFO : setting ignored attribute cum_table to None\n",
      "2018-04-03 12:15:17,320 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'syn0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-16df20daba9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'syn0'"
     ]
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       \n",
    "       # Print a status message every 1000th review\n",
    "           \n",
    "           \n",
    "           # Call the function (defined above) that makes average feature vectors\n",
    "            reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "               num_features)\n",
    "           \n",
    "            counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'index2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-bddcc186c7ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainDataVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating average feature vecs for test reviews\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-4e802e05cdb4>\u001b[0m in \u001b[0;36mgetAvgFeatureVecs\u001b[0;34m(reviews, model, num_features)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m            \u001b[0;31m# Call the function (defined above) that makes average feature vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mreviewFeatureVecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeFeatureVec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-1833a5bc26a6>\u001b[0m in \u001b[0;36mmakeFeatureVec\u001b[0;34m(words, model, num_features)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Index2word is a list that contains the names of the words in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# the model's vocabulary. Convert it to a set, for speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mindex2word_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Loop over each word in the review and, if it is in the model's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'index2word'"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in rev_train:\n",
    "    clean_train_reviews.append( review_to_wordlist( str(review), remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print (\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in rev_train:\n",
    "    clean_test_reviews.append( review_to_wordlist( str(review), \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainDataVecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-176de0f71497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainDataVecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainDataVecs' is not defined"
     ]
    }
   ],
   "source": [
    "trainDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainDataVecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-d97638efe85a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2908\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDataVecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mtrainDataVecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainDataVecs' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(2908):\n",
    "    for j in range(300):\n",
    "        if(np.isnan(trainDataVecs[i][j])):\n",
    "            trainDataVecs[i][j]=0\n",
    "            print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainDataVecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-043330b7ad2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainDataVecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDataVecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrev_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainDataVecs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = trainDataVecs[train_index], trainDataVecs[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(svm.SVC(kernel='rbf').fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = trainDataVecs[train_index], trainDataVecs[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(svm.SVC(kernel='linear').fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "neigh = KNeighborsClassifier(n_neighbors=35)\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = trainDataVecs[train_index], trainDataVecs[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(neigh.fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = trainDataVecs[train_index], trainDataVecs[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X_train, y_train)\n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(bnb.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "avg_score=0\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = trainDataVecs[train_index], trainDataVecs[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    forest = RandomForestClassifier(n_estimators = 80) \n",
    "    forest = forest.fit( X_train, y_train )\n",
    "    #YPred = forest.predict(X_test)\n",
    "    score = forest.score(X_test,y_test)\n",
    "    avg_score += score \n",
    "    #y.append(YPred)\n",
    "    print(score)\n",
    "    #y_test1=[]\n",
    "    #for ii in range(y_test.size):\n",
    "    #    if (y_test[ii] == 'N'):\n",
    "    #        y_test1.append(0)\n",
    "    #    else:\n",
    "    #        y_test1.append(1)\n",
    "    #YPred1=[]\n",
    "    #for ii in range(YPred.size):\n",
    "    #    if (YPred[ii] == 'N'):\n",
    "    #        YPred1.append(0)\n",
    "    #    else:\n",
    "    #        YPred1.append(1)    \n",
    "    #fpr, tpr, thresholds = metrics.roc_curve(y_test1, YPred1, pos_label=2)\n",
    "    #print(\"AUC\",metrics.auc(fpr, tpr))\n",
    "    #print(YPred.shape)\n",
    "    #Err=0\n",
    "    #for i in range(YPred.size):\n",
    "     #   if YPred[i] != y_test[i]:\n",
    "          #  Err+=1\n",
    "            #print(YPred[i], y_test[i])\n",
    "    #print (Err)\n",
    "print(\"avg\",avg_score/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = trainDataVecs[train_index], trainDataVecs[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(svm.LinearSVC().fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = trainDataVecs[train_index], trainDataVecs[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    #YPred = forest.predict(X_test)\n",
    "    #y.append(YPred)\n",
    "    print(logreg.fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Reviewer's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('yelp_data_reviewer.dat', sep='\\t')\n",
    "df1 = np.array(df1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_name = []\n",
    "for i in range(len(df1)):\n",
    "    var=df1[i][0].split(';')\n",
    "    rev_name.append(var[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_id = []\n",
    "for i in range(len(df1)):\n",
    "    var=df1[i][0].split(';')\n",
    "    rev_id.append(var[0:1])\n",
    "rev_id=np.array(rev_id)\n",
    "rev_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_name=np.array(rev_name)\n",
    "rev_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_name_train=[]\n",
    "for i in range(2908):\n",
    "    temp = rev_id_train[i][0]\n",
    "    for j in range (5123):\n",
    "        c=0\n",
    "        if(temp == rev_id[j][0]):\n",
    "            rev_name_train.append(rev_name[j][0])\n",
    "            c=1\n",
    "            break\n",
    "    if c==0:\n",
    "        rev_name_train.append(\"Fake Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rev_name_train = np.array(rev_name_train)\n",
    "rev_name_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rev_conc=np.concatenate((rev_name_train,rev_train),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rev_conc=np.array(rev_conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Reading the given paper we draw the following conclusions\n",
    "## 1. Majority fake reviews are less than 135 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_len=[]\n",
    "for i in range(2908):\n",
    "    rev_len.append(len(rev_train[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[]\n",
    "for i in range(2908):\n",
    "    count=0\n",
    "    temp=rev_name_train[i][:]\n",
    "    for j in range(2908):\n",
    "        if (rev_name_train[i][:]==rev_name_train[j][:] and rev_date_train[i][:] ==rev_date_train[j][:] and j!=i):\n",
    "            #print(i,j)\n",
    "            count+=1\n",
    "    #if(count>=3):\n",
    "        #print(i)\n",
    "    c.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=np.array(c)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### This shows no reviewer has made more than 5 comments on one day so we cannot verufy the analysis from yelp paper that 75% of spammers posted more than 5 reviews per day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_len= np.array(rev_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_len.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_len.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_chk=0\n",
    "for i in range(len(rev_len)):\n",
    "    if rev_len[i] <=135:\n",
    "        count_chk +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_chk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We see that 175 reviews are less than 135 characters hence those 175 reviews have more chances to be fake reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now using this behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "n_folds = 5\n",
    "score = 0.0\n",
    "skf = StratifiedKFold(rev_test, n_folds)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = train_data_features[train_index], train_data_features[test_index]\n",
    "    y_train, y_test = rev_test[train_index], rev_test[test_index]\n",
    "    \n",
    "    \n",
    "    YPred = svm.SVC(kernel='sigmoid').fit(X_train, y_train).predict(X_test)\n",
    "    YPred = np.array(YPred)\n",
    "    Ylen = rev_len[test_index]\n",
    "    \n",
    "    sc=0\n",
    "    for h in range(len(test_index)):\n",
    "        if (YPred[h] != y_test[h]):\n",
    "            sc+=1\n",
    "    print((len(test_index)-sc)/len(test_index))\n",
    "    \n",
    "    for h in range(len(test_index)):\n",
    "        if (YPred[h] == 'N' and Ylen[h] <135):\n",
    "            #print('Shift')\n",
    "            YPred[h] = 'Y'\n",
    "    sc=0\n",
    "    for h in range(len(test_index)):\n",
    "        if (YPred[h] != y_test[h]):\n",
    "            sc+=1\n",
    "    print((len(test_index)-sc)/len(test_index),'Next')        \n",
    "\n",
    "    #y.append(YPred)\n",
    "    #print(svm.SVC(kernel='sigmoid').fit(X_train, y_train).score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
