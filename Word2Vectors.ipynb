{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk.data\n",
    "#nltk.download()   \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('cve-2016.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9417, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CVE_Items', 'CVE_data_format', 'CVE_data_numberOfCVEs',\n",
       "       'CVE_data_timestamp', 'CVE_data_type', 'CVE_data_version'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = []\n",
    "severity = []\n",
    "scores = []\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    new=dataset.CVE_Items[i]\n",
    "    if('baseMetricV2' in new['impact'].keys()):\n",
    "        severity.append(new['impact']['baseMetricV2']['severity'])\n",
    "        scores.append(new['impact']['baseMetricV2']['cvssV2']['baseScore'])\n",
    "        description.append(new['cve']['description']['description_data'][0]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = np.array(description)\n",
    "severity = np.array(severity)\n",
    "scores = np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "def review_to_wordlist( raw_review, remove_stopwords=False  ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(words ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "\n",
    "for review in description:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11870"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibm',\n",
       " 'sametime',\n",
       " 'meeting',\n",
       " 'server',\n",
       " 'vulnerable',\n",
       " 'cross',\n",
       " 'site',\n",
       " 'scripting']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[3400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8205,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-09 02:03:02,365 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2018-07-09 02:03:02,371 : INFO : collecting all words and their counts\n",
      "2018-07-09 02:03:02,372 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-09 02:03:02,424 : INFO : PROGRESS: at sentence #10000, processed 202304 words, keeping 9499 word types\n",
      "2018-07-09 02:03:02,434 : INFO : collected 10441 word types from a corpus of 230372 raw words and 11870 sentences\n",
      "2018-07-09 02:03:02,435 : INFO : Loading a fresh vocabulary\n",
      "2018-07-09 02:03:02,442 : INFO : min_count=40 retains 705 unique words (6% of original 10441, drops 9736)\n",
      "2018-07-09 02:03:02,442 : INFO : min_count=40 leaves 189168 word corpus (82% of original 230372, drops 41204)\n",
      "2018-07-09 02:03:02,447 : INFO : deleting the raw counts dictionary of 10441 items\n",
      "2018-07-09 02:03:02,448 : INFO : sample=0.001 downsamples 77 most-common words\n",
      "2018-07-09 02:03:02,449 : INFO : downsampling leaves estimated 127140 word corpus (67.2% of prior 189168)\n",
      "2018-07-09 02:03:02,454 : INFO : estimated required memory for 705 words and 300 dimensions: 2044500 bytes\n",
      "2018-07-09 02:03:02,455 : INFO : resetting layer weights\n",
      "2018-07-09 02:03:02,477 : INFO : training model with 4 workers on 705 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-09 02:03:02,613 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-09 02:03:02,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-09 02:03:02,621 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-09 02:03:02,624 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-09 02:03:02,625 : INFO : EPOCH - 1 : training on 230372 raw words (126976 effective words) took 0.1s, 936605 effective words/s\n",
      "2018-07-09 02:03:02,741 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-09 02:03:02,743 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-09 02:03:02,744 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-09 02:03:02,749 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-09 02:03:02,750 : INFO : EPOCH - 2 : training on 230372 raw words (127006 effective words) took 0.1s, 1090665 effective words/s\n",
      "2018-07-09 02:03:02,861 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-09 02:03:02,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-09 02:03:02,870 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-09 02:03:02,871 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-09 02:03:02,872 : INFO : EPOCH - 3 : training on 230372 raw words (127203 effective words) took 0.1s, 1109499 effective words/s\n",
      "2018-07-09 02:03:02,976 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-09 02:03:02,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-09 02:03:02,982 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-09 02:03:02,984 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-09 02:03:02,985 : INFO : EPOCH - 4 : training on 230372 raw words (126960 effective words) took 0.1s, 1228739 effective words/s\n",
      "2018-07-09 02:03:03,098 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-09 02:03:03,105 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-09 02:03:03,108 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-09 02:03:03,110 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-09 02:03:03,111 : INFO : EPOCH - 5 : training on 230372 raw words (127279 effective words) took 0.1s, 1078009 effective words/s\n",
      "2018-07-09 02:03:03,112 : INFO : training on a 1151860 raw words (635424 effective words) took 0.6s, 1001218 effective words/s\n",
      "2018-07-09 02:03:03,113 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-07-09 02:03:03,124 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-07-09 02:03:03,125 : INFO : not storing attribute vectors_norm\n",
      "2018-07-09 02:03:03,126 : INFO : not storing attribute cum_table\n",
      "2018-07-09 02:03:03,152 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gold', 0.9094369411468506),\n",
       " ('rt', 0.8332555890083313),\n",
       " ('loading', 0.827616274356842),\n",
       " ('r', 0.7401587963104248),\n",
       " ('continuous', 0.7247640490531921),\n",
       " ('microsoft', 0.7045366168022156),\n",
       " ('os', 0.690426766872406),\n",
       " ('sp', 0.6736881732940674),\n",
       " ('execute', 0.6600849628448486),\n",
       " ('privileges', 0.6469964981079102)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caused', 0.8722426891326904),\n",
       " ('forgery', 0.8688178062438965),\n",
       " ('cross', 0.855127215385437),\n",
       " ('websphere', 0.8501957654953003),\n",
       " ('csrf', 0.7897992134094238),\n",
       " ('reflected', 0.7893112897872925),\n",
       " ('lifecycle', 0.7889853715896606),\n",
       " ('tivoli', 0.7777188420295715),\n",
       " ('actions', 0.776308536529541),\n",
       " ('victim', 0.7574336528778076)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"vulnerable\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=705, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['internet'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04629976 -0.03775239 -0.0612707   0.04138781  0.01347958 -0.13033734\n",
      "  0.03593222 -0.09561654  0.09193933  0.07477549 -0.01045573 -0.06526111\n",
      "  0.04529662  0.00507673 -0.05801545 -0.08556713 -0.03978465  0.00544046\n",
      "  0.05104588 -0.06756661  0.0370345  -0.04885554 -0.06992226  0.06885462\n",
      " -0.01036942 -0.01555624  0.01590928  0.05672768  0.03949884  0.00494205\n",
      "  0.09041023  0.00571667  0.09692305  0.00783475 -0.01307637 -0.09706757\n",
      "  0.01655647 -0.05390599 -0.04951017 -0.0244361  -0.03356093 -0.05495088\n",
      " -0.09433108 -0.01117052  0.01180344  0.06742935  0.02205241 -0.01068878\n",
      "  0.03313516  0.05438668  0.07552817 -0.09990546 -0.08005269  0.04091886\n",
      "  0.07880871 -0.04165225 -0.00918824  0.11682916  0.01531674  0.03951841\n",
      "  0.06148017 -0.03348539  0.07114412 -0.05529675  0.02345757  0.05184504\n",
      " -0.08638492  0.03308222 -0.04185783  0.06531723 -0.02965119  0.05407631\n",
      " -0.08639693  0.01736786  0.0844368   0.03411895 -0.04996516 -0.04844293\n",
      " -0.00310361  0.11538444 -0.05378686 -0.02967841  0.07217427 -0.00684789\n",
      " -0.0179275   0.00763353  0.09376784 -0.02292064  0.00583797 -0.05738004\n",
      "  0.05958892 -0.00936469 -0.0802224   0.02582087 -0.01356882  0.05525155\n",
      " -0.03021552  0.05766937  0.07452633 -0.03261406  0.01363617  0.02784311\n",
      "  0.05403625 -0.0721683  -0.07162187  0.07456078 -0.05203228  0.0164813\n",
      "  0.0517411  -0.04355783  0.02554489  0.03528553 -0.01070342  0.02726801\n",
      " -0.02041939 -0.05251348  0.08820312  0.01178347  0.00489954  0.00588435\n",
      "  0.01646947  0.07780317  0.09179305  0.11958703 -0.0700468   0.08392422\n",
      " -0.10074809 -0.0024156  -0.03128896  0.05947265 -0.02526046 -0.07388628\n",
      " -0.01059673 -0.02448021  0.01914293 -0.08530535 -0.0962625   0.02729028\n",
      "  0.08145312 -0.0462271  -0.06010916 -0.09902808 -0.05604574  0.09974051\n",
      " -0.04245941  0.08706457  0.07051359 -0.00488337 -0.04609452  0.1145611\n",
      " -0.0057279  -0.03807061 -0.05118256 -0.11710254  0.02467629  0.06788233\n",
      " -0.06110778 -0.01176782  0.00729163 -0.03396017 -0.0781162  -0.06268714\n",
      "  0.03930313 -0.03695241 -0.02600467 -0.12235393  0.02224673  0.07157399\n",
      " -0.05865877  0.00422364 -0.0476161   0.07807697  0.16394554 -0.03288585\n",
      " -0.04041051 -0.0528004  -0.0232111  -0.02279345  0.00045042  0.09485012\n",
      "  0.03876022 -0.05911956  0.01148629 -0.00197921  0.05525924 -0.03475343\n",
      "  0.04871691 -0.01206489 -0.02855589  0.01153949  0.01555656  0.03300145\n",
      " -0.03109088 -0.03301695  0.07278996  0.01285422  0.00177927 -0.0844135\n",
      "  0.11162003  0.07612702  0.02196101  0.0828164  -0.11434157 -0.0276711\n",
      "  0.07023861  0.10799347  0.03814689 -0.00362443 -0.08742201 -0.07609434\n",
      "  0.00698442  0.04796709 -0.03997444 -0.00326881  0.08609959  0.09073488\n",
      " -0.0240713   0.03292091 -0.03615429  0.0610457  -0.10379627 -0.02782106\n",
      " -0.00432852  0.07537484  0.05877474 -0.00925471 -0.03437344 -0.0141502\n",
      "  0.04523747 -0.09719168 -0.04366856 -0.030666   -0.05388606  0.02842467\n",
      " -0.04205393  0.04848009 -0.12231693  0.03359671  0.05251773  0.00452684\n",
      " -0.02673173 -0.04534775 -0.04329343  0.04071627 -0.04099033 -0.00925254\n",
      "  0.00621853 -0.03125075  0.08599172 -0.03397847 -0.08195248 -0.05399286\n",
      "  0.03652589  0.00599218  0.07541826 -0.074653   -0.00042468  0.06669124\n",
      " -0.00104121  0.071053    0.00749078 -0.03258489  0.02988847 -0.09998156\n",
      " -0.11790653  0.09019578  0.11468264 -0.07821774 -0.01264623  0.04464556\n",
      " -0.01605871  0.04245372 -0.03344565 -0.08192168 -0.00658706 -0.08812152\n",
      "  0.01667917 -0.04725152 -0.02900057  0.00827639  0.01228501  0.08989947\n",
      "  0.02437822  0.01969418 -0.06138816  0.03426072  0.02008862 -0.06260328\n",
      " -0.1004454   0.05077861  0.08392266  0.06744874  0.03703656 -0.02402286\n",
      "  0.06734611  0.08680253  0.09700686  0.02178777 -0.02805798 -0.00768509]\n"
     ]
    }
   ],
   "source": [
    "print(model[words[704]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
